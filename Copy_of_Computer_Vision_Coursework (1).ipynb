{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZjxx15xr-T"
      },
      "source": [
        "We will start by setting up our environment on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puWvCgBgTvEZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hB7qgQxuxsyj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edvQdR0ox_bi"
      },
      "source": [
        "We will now recall the path in our Google Drive where we uploaded the coursework materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I6gLwvNx8C7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/CompVision_Labs/Computer Vision Coursework'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwIop3xjE5g6"
      },
      "source": [
        "We will now define a function to indicate the images and labels path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "limPdQt6Xbas"
      },
      "outputs": [],
      "source": [
        "images_folder = os.path.join(GOOGLE_DRIVE_PATH,'CV2024_CW_Dataset (1)/train/images')\n",
        "labels_folder = os.path.join(GOOGLE_DRIVE_PATH,'CV2024_CW_Dataset (1)/train/labels')\n",
        "\n",
        "\n",
        "print(images_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X15W4nwiHWrS"
      },
      "source": [
        "We will now assign the images and labels files to lists of images and files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUsNLkg-HWG_"
      },
      "outputs": [],
      "source": [
        "images_files = os.listdir(images_folder)\n",
        "labels_files = os.listdir(labels_folder)\n",
        "\n",
        "print(images_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3LlBrNa071t"
      },
      "source": [
        "We will now build the path for the test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iU2hSpU0_AV"
      },
      "outputs": [],
      "source": [
        "test_img_path = os.path.join(GOOGLE_DRIVE_PATH,'CV2024_CW_Dataset (1)/test/images')\n",
        "test_img_files = os.listdir(os.path.join(GOOGLE_DRIVE_PATH,'CV2024_CW_Dataset (1)/test/images'))\n",
        "test_lbl_files = os.listdir(os.path.join(GOOGLE_DRIVE_PATH,'CV2024_CW_Dataset (1)/test/labels'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYR_AT3Taq1-"
      },
      "source": [
        "We will first upload, resize and normalize all the images in the Dataset. We will turn them into grayscale in order to simplify our analysis and because our chosen descriptors operate on grayscale images. We will open each Image in the Image list, transform it, assign each image to the new images list and create the label list in paralel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KK24PaCbSGQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from skimage import io, color, img_as_ubyte\n",
        "#We will define the transformations we want to appluy for each image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        " ])\n",
        "\n",
        "#Initiate empty lists to store the transformed images and labels\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "#We will Iterate through the list of image files\n",
        "\n",
        "for image_file in images_files:\n",
        "  #Construct the full path to the image\n",
        "  image_path = os.path.join(images_folder, image_file)\n",
        "  #Open the image\n",
        "  image = Image.open(image_path)\n",
        "  #Transform the image\n",
        "  image = transform(image)\n",
        "\n",
        "\n",
        "  #Append the transformed image to the list\n",
        "  images.append(image)\n",
        "\n",
        "  #Build the path to the label file\n",
        "  label_file = os.path.splitext(image_file)[0] + \".txt\"\n",
        "  label_path = os.path.join(labels_folder, label_file)\n",
        "\n",
        "  with open(label_path, 'r') as f:\n",
        "    label = f.read().strip()\n",
        "    labels.append(label)\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Images\", images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAZ6bauhySF"
      },
      "source": [
        "We will now transform the labels in the lsit to numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WBHgZfxh1_h"
      },
      "outputs": [],
      "source": [
        "labels = [int(item) for item in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU_S47ghh9IG"
      },
      "outputs": [],
      "source": [
        "for item in labels:\n",
        "  print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "romcx2qk-cus"
      },
      "outputs": [],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUg_vqTyRk_t"
      },
      "source": [
        "After we obtained 2 separate lists with the transformed images and labels, we will create tuples of images and labels so we correlate them before applying our classification models. Each tuple will contain the transformed image and the corresponding label. We will use zip to make the process more efficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9tUlPfjRt5_"
      },
      "outputs": [],
      "source": [
        "data = list(zip(images, labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-dQuLd2GuW"
      },
      "source": [
        "We will do the same for test images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEsH8YJrR9ZS"
      },
      "source": [
        "We will now apply 2 different descriptors that we will use for face recognition: HOG and LBP descriptors to each image in the dataset. We will create two lists containing the HOG and the LBP descriptors of each image so that we can use both for our classification models and see which works best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2veh84VSTCbc"
      },
      "source": [
        "LBP descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHZ2cM6-THUF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from skimage import feature\n",
        "import numpy as np\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "\n",
        "\n",
        "#Compute the LBP descriptor\n",
        "def compute_lbp(image):\n",
        "  #Parameters for LBP computation\n",
        "  radius = 3\n",
        "  n_points = 8 * radius\n",
        "  method = 'uniform'\n",
        "\n",
        "  #Compute the LBP descriptor\n",
        "  lbp = local_binary_pattern(image, n_points, radius, method)\n",
        "\n",
        "  #Calculate histogram of LBP image\n",
        "  hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "\n",
        "  #Normalize histogram\n",
        "  hist = hist.astype(\"float\")\n",
        "  hist /= (hist.sum() + 1e-7)\n",
        "\n",
        "  return hist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q5FB01XZAxl"
      },
      "source": [
        "We will now store descriptors of each image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAyda3tndBBx"
      },
      "outputs": [],
      "source": [
        "images_lbp = []\n",
        "\n",
        "for image, label in data:\n",
        "  #we will first transform image to numpy array\n",
        "  image_np = image.numpy().squeeze().astype(np.uint8)\n",
        "\n",
        "  #Compute LBP descriptor\n",
        "  image_lbp = compute_lbp(image_np)\n",
        "  images_lbp.append(image_lbp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aVeeYJyg0cq"
      },
      "source": [
        "We will now make tuples of the image descriptors and labels for each descriptor before training our SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHYOtcgchIIG"
      },
      "outputs": [],
      "source": [
        "data_lbp = list(zip(images_lbp, labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGz7KE2Ijb8L"
      },
      "source": [
        "We will now extract the descriptors for each image and train the SVM model. Here, image descriptors are the predictors while the label us the target variable. Basic SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEzXJpEWoNX1"
      },
      "source": [
        "We will now build the model for the LBP descriptors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMDY-kwNoLdy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X = [image_lbp for image_lbp, label in data_lbp]\n",
        "y = [label for image_lbp, label in data_lbp]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "svm_model = SVC(kernel='linear', decision_function_shape='ovr')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(\"Training Time:\", training_time, \"seconds\")\n",
        "\n",
        "accuracy = svm_model.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Compute performance metrics\n",
        "precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
        "recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
        "f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1_score)\n",
        "\n",
        "#We will now build a Confusion Matrix to asses model Performance\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cPCyacpWs9c"
      },
      "source": [
        "Now I will Build my MLP model with one hidden Layer for the analysis. Because we have a 3 class classification task, we will use the softmax activation function for the output layer and the rectified linear unit activation function between the hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkqebAoBXw6C"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class MLP1(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size1, output_size):\n",
        "    super(MLP1, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size1, output_size)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJ1ZGiNfd6R"
      },
      "source": [
        "We will now implement an MLP with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fkaIY1Ffhye"
      },
      "outputs": [],
      "source": [
        "class MLP2(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "    super(MLP2, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(hidden_size2, output_size)\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZVGJNRef8uP"
      },
      "source": [
        "We will now initialize the model and set the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7guDOICqgB6_"
      },
      "outputs": [],
      "source": [
        "learning_rate = [0.001, 0.01, 0.1 ]\n",
        "momentum = [0.5, 0.95]\n",
        "weight_decay = (0.0, 0.1)\n",
        "hidden_size1 = [64, 128, 256]\n",
        "hidden_size2 = [64, 128, 256]\n",
        "\n",
        "\n",
        "output_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrji3_AQk4HK"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s27I9_9nn3F"
      },
      "source": [
        "We will do the same for the LBP descriptor images with the MLP with one layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRLisg-onrcg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "X = [image_lbp for image_lbp, label in data_lbp]\n",
        "y = [label for image_lbp, label in data_lbp]\n",
        "y = np.array(y, dtype=np.int32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "\n",
        "best_accuracy = 0.0\n",
        "best_hyperparameters = None\n",
        "\n",
        "# Lists to store results for plotting\n",
        "accuracy_values = []\n",
        "\n",
        "\n",
        "for i, lr in enumerate(learning_rate):\n",
        "  for m, mt in enumerate(momentum):\n",
        "    for j, hs1 in enumerate(hidden_size1):\n",
        "      for w, wd in enumerate(weight_decay):\n",
        "        MLP1_model = MLP1(input_size, hs1, output_size)  # Create a new model for each combination of hyperparameters\n",
        "        optimizer = optim.SGD(MLP1_model.parameters(), lr=lr, momentum=mt, weight_decay=wd)\n",
        "\n",
        "        MLP1_model.train()\n",
        "\n",
        "        epochs = 50\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          y_pred = MLP1_model(X_train)\n",
        "          loss = criterion(y_pred, y_train)\n",
        "\n",
        "          #Backpropagation\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "                  print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        MLP1_model.eval()\n",
        "        with torch.no_grad():\n",
        "          outputs = MLP1_model(X_test)\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
        "          accuracy_values.append(accuracy)\n",
        "          precision = precision_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "          recall = recall_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "          f1 = f1_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "          print(f'Accuracy on the test set: {accuracy:.4f}')\n",
        "          print(f'Precision: {precision:.4f}')\n",
        "          print(f'Recall: {recall:.4f}')\n",
        "          print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "          conf_matrix = confusion_matrix(y_test.numpy(), predicted.numpy())\n",
        "          # Plot confusion matrix\n",
        "          plt.figure(figsize=(8, 6))\n",
        "          sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "          plt.title(\"Confusion Matrix\")\n",
        "          plt.xlabel(\"Predicted Label\")\n",
        "          plt.ylabel(\"True Label\")\n",
        "          plt.show()\n",
        "\n",
        "          if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparameters = (lr, mt, hs1, wd)\n",
        "\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "plt.plot(accuracy_values)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy of Models')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pYPsizLpAZV"
      },
      "source": [
        "We will now train the MLP with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IoUU0sapDp7"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0.0\n",
        "accuracy_values = []\n",
        "precision_values = []\n",
        "recall_values = []\n",
        "f1_values = []\n",
        "best_hyperparameters = None\n",
        "for i, lr in enumerate(learning_rate):\n",
        "  for m, mt in enumerate(momentum):\n",
        "    for j1, hs1 in enumerate(hidden_size1):\n",
        "      for j2, hs2 in enumerate(hidden_size2):\n",
        "        for w, wd in enumerate(weight_decay):\n",
        "          MLP2_model = MLP2(input_size, hs1, hs2, output_size)  # Create a new model for each combination\n",
        "          optimizer = optim.SGD(MLP2_model.parameters(), lr=lr, momentum=mt, weight_decay=wd)\n",
        "\n",
        "          MLP2_model.train()\n",
        "\n",
        "          epochs = 50\n",
        "\n",
        "          for epoch in range(epochs):\n",
        "            y_pred = MLP2_model(X_train)\n",
        "            loss = criterion(y_pred, y_train)\n",
        "\n",
        "          #Backpropagation\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "          MLP2_model.eval()\n",
        "          with torch.no_grad():\n",
        "            outputs = MLP2_model(X_test)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            accuracy = accuracy_score(y_test.numpy(), predicted.numpy())\n",
        "            accuracy_values.append(accuracy)\n",
        "            precision = precision_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "            recall = recall_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "            f1 = f1_score(y_test.numpy(), predicted.numpy(), average='weighted')\n",
        "\n",
        "            print(f'Accuracy on the test set: {accuracy:.4f}')\n",
        "            print(f'Precision: {precision:.4f}')\n",
        "            print(f'Recall: {recall:.4f}')\n",
        "            print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "            conf_matrix = confusion_matrix(y_test.numpy(), predicted.numpy())\n",
        "            # Plot confusion matrix\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "            plt.title(\"Confusion Matrix\")\n",
        "            plt.xlabel(\"Predicted Label\")\n",
        "            plt.ylabel(\"True Label\")\n",
        "            plt.show()\n",
        "\n",
        "            if accuracy > best_accuracy:\n",
        "              best_accuracy = accuracy\n",
        "              best_hyperparameters = (lr, mt, hs1, hs2, wd)\n",
        "\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "plt.plot(accuracy_values)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy of Models')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ax5Hi05hpzKi"
      },
      "source": [
        "We will now run a Convolutional Neural Network (CNN) on our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LZtCgkE5Q_S"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.fc3 = nn.Linear(64, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(torch.relu(self.conv1(x)))\n",
        "    x = self.pool(torch.relu(self.conv2(x)))\n",
        "    x = x.view(-1, 64 * 56 * 56)\n",
        "    x = torch.relu(self.fc1(x))\n",
        "    x = torch.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn_izIL2C89f"
      },
      "source": [
        "We will now train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf9g1mC7DM6w"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXWB12_RdfTX"
      },
      "source": [
        "Nowe we will prepare the Data to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rDmfXtkdhna"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from PIL import Image\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "##We will convert lists of images and tables in tensors\n",
        "\n",
        "images_tensor = torch.stack(images)\n",
        "labels_tensor = torch.tensor(labels)\n",
        "\n",
        "\n",
        "dataset = TensorDataset(images_tensor, labels_tensor)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHgG5_vsHBCE"
      },
      "outputs": [],
      "source": [
        "print(\"Number of images:\", images_tensor.size(0))\n",
        "print(\"Number of labels:\", labels_tensor.size(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now train our CNN model and Display the accuracy and the Confusion Matrix"
      ],
      "metadata": {
        "id": "yv_LUHktPlvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "epochs = 2\n",
        "\n",
        "accuracy_list = []\n",
        "conf_matrices = []\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics (loss.item() returns the mean loss in the mini-batch)\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # Print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # After each epoch, evaluate the model on the test set\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Append true and predicted labels for confusion matrix\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        accuracy_list.append(accuracy)\n",
        "        print('Accuracy after epoch %d: %f %%' % (epoch + 1, accuracy))\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "        conf_matrices.append(conf_matrix)\n",
        "\n",
        "print('Finished Training')\n",
        "print('Training Time: %s seconds' % (time.time() - t0))\n",
        "\n",
        "# Plot the accuracy development score\n",
        "plt.plot(accuracy_list)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Development')\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix for the last epoch\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrices[-1], annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXWW2IF3PkvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfvtsVY7gCA1"
      },
      "source": [
        "Mask Detection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1t7cO-PvLMn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def mask_detection(path_to_testset, model_type):\n",
        "  test_files = os.listdir(path_to_testset)\n",
        "  selected_files = np.random.choice(test_files, 4, replace=False)\n",
        "  #Display each selected image with its prediction\n",
        "\n",
        "  for image_file in selected_files:\n",
        "        img_path = os.path.join(path_to_testset, image_file)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        #Preprocess images\n",
        "\n",
        "        img_transformed = transform(img)\n",
        "\n",
        "        if model_type == \"svm_model\":\n",
        "          img_np = img_transformed.numpy().squeeze().astype(np.uint8)\n",
        "          img_lbp = compute_lbp(img_np)\n",
        "          image = img_lbp\n",
        "          model = svm_model\n",
        "          image = image.reshape(1, -1)\n",
        "          prediction = svm_model.predict(image)\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Prediction: {prediction}, Filename: {image_file}\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "        elif model_type == \"MLP1_model\":\n",
        "          img_np = img_transformed.numpy().squeeze().astype(np.uint8)\n",
        "          img_lbp = compute_lbp(img_np)\n",
        "          img_torch = torch.tensor(img_lbp, dtype=torch.float32)\n",
        "          image = img_torch\n",
        "          model = MLP1_model\n",
        "          output = MLP1_model(image)\n",
        "          _, prediction = torch.max(output, 0)\n",
        "          prediction = prediction.item()\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Prediction: {prediction}, Filename: {image_file}\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "        elif model_type == \"net\":\n",
        "          image = img_transformed\n",
        "          model = net\n",
        "          output = net(image)\n",
        "          _, prediction = torch.max(outputs, 1)\n",
        "          prediction = prediction.item()\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Prediction: {prediction}, Filename: {image_file}\")\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "\n",
        "        else:\n",
        "          raise ValueError(\"Invalid model type specified.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6IF5X2kAJhV"
      },
      "source": [
        "We will now test the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHQt1Hl9AIEX"
      },
      "outputs": [],
      "source": [
        "mask_detection(test_img_path, model_type=\"net\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zobR6RkegWsL"
      },
      "source": [
        "Mask Detection Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZdT-snLKQst"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def MaskDetectionVideo(video_path):\n",
        "  #Load the video\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "  #We will use the pretrained face detection algorithm using the Haar cascade classifier.\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "  #Define a function to process each frame\n",
        "  for frame in range(total_frames):\n",
        "    #Process every 100th frame\n",
        "    if frame % 100 == 0:\n",
        "      ret, img = cap.read()\n",
        "      if ret:\n",
        "        #Mask Detection Pipeline using our trained model for CNN\n",
        "        img_pil = Image.fromarray(img)\n",
        "        img_transformed = transform(img_pil)\n",
        "        output = net(img_transformed)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "        prediction = prediction.item()\n",
        "        #We will now draw a bounding box and label the image\n",
        "        if prediction == 0:\n",
        "          label = \"No Mask\"\n",
        "          color= (0, 0, 255)\n",
        "        elif prediction == 1:\n",
        "          label = \"Mask\"\n",
        "          color = (0, 0, 255)\n",
        "        else:\n",
        "          label = \"Mask Improperly\"\n",
        "          color = (0, 0, 255)\n",
        "\n",
        "        #Now we will detect faces and draw the bounding box and post the labels\n",
        "\n",
        "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        #Detect faces\n",
        "        faces = face_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "        #We will loov through each detected face\n",
        "        for (x, y, w, h) in faces:\n",
        "          # Draw bounding box\n",
        "          cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
        "\n",
        "          # Add label\n",
        "          cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "        # Display the processed frame\n",
        "        cv2_imshow(img)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "  # Release the video capture object\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOomtKhQiVj"
      },
      "source": [
        "Video Function 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_LA0ZNGQlS7"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def MaskDetectionVideo(video_path):\n",
        "    # Load the video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # We will use the pretrained face detection algorithm using the Haar cascade classifier.\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "    # Define a function to process each frame\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, img = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 10 != 0:\n",
        "            continue\n",
        "\n",
        "        # Mask Detection Pipeline using our trained model for CNN\n",
        "        img_pil = Image.fromarray(img)\n",
        "        img_transformed = transform(img_pil)\n",
        "        output = net(img_transformed)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "        prediction = prediction.item()\n",
        "\n",
        "        # We will now draw a bounding box and label the image\n",
        "        if prediction == 0:\n",
        "            label = \"No Mask\"\n",
        "            color = (0, 0, 255)\n",
        "        elif prediction == 1:\n",
        "            label = \"Mask\"\n",
        "            color = (0, 0, 255)\n",
        "        else:\n",
        "            label = \"Mask Improperly\"\n",
        "            color = (0, 0, 255)\n",
        "\n",
        "        # Now we will detect faces and draw the bounding box and post the labels\n",
        "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces\n",
        "        faces = face_cascade.detectMultiScale(img_gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "        # Loop through each detected face\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(img, (x, y), (x+w, y+h), color, 2)\n",
        "\n",
        "            # Add label\n",
        "            cv2.putText(img, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "        # Display the processed frame\n",
        "        cv2_imshow(img)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8CgjSifIlxI"
      },
      "source": [
        "We will now try our Mask Detection Video Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUtNPZSmIpFv"
      },
      "outputs": [],
      "source": [
        "video_path = os.path.join(GOOGLE_DRIVE_PATH, 'videoplayback.mp4')\n",
        "\n",
        "MaskDetectionVideo(video_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}